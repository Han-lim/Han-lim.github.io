---
published: true
title:  "[Paper Review] FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection"
categories:
  - 3d_object_detection
tags:
  - [3D_Object_Detection, Computer_Vision, Deep_Learning, Paper_Review]

toc: true
toc_sticky: true
use_math: true
 
date: 2022-06-24
last_modified_at: 2020-06-24
---
ë…¼ë¬¸ ì •ë³´: Wang, T., Zhu, X., Pang, J., & Lin, D. (2021). Fcos3d: Fully convolutional one-stage monocular 3d object detection. InÂ Proceedings of the IEEE/CVF International Conference on Computer Vision
Â (pp. 913-922).

---

## Abstract

ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, fully convolutional single-stage detector êµ¬ì¡°ë¥¼ í™œìš©í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì´ ë¬¸ì œë¥¼ ì—°êµ¬í•˜ê³ , general framework FCOS 3Dë¥¼ ì œì•ˆí•˜ì˜€ìŠµë‹ˆë‹¤. <br>
ì „ì²´ì ì¸ pipelineì€, <br>
(1) ë¨¼ì € 7-DoF 3D targetë“¤ì„ image domainìœ¼ë¡œ transform í•œ í›„, <br>
(2) transformí•œ targetë“¤ì„ 2Dì™€ 3D attributeìœ¼ë¡œ decouple <br>
(3) objectë“¤ì„ 2D scaleì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ feature levelë¡œ ë§Œë“  í›„, <br>
(4) projected 3D-centerì— ëŒ€í•´ì„œë§Œ ìœ„ ê²°ê³¼ë¥¼ assign <br>

center-nessëŠ” 3D target formulation ì— ë§ì¶”ê¸° ìœ„í•´ 3D-center ê¸°ë°˜ 2D Gaussian distributionìœ¼ë¡œ ë‹¤ì‹œ ì •ì˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ frameworkì€ ë‹¨ìˆœí•˜ì§€ë§Œ, íš¨ìœ¨ì ìœ¼ë¡œ ì´ì „ì²˜ëŸ¼ 2D detectorë‚˜ 2D-3D correspondence ë‹¨ê³„ë¥¼ ì—†ì•´ìŠµë‹ˆë‹¤. ë˜í•œ, NeurIPS 2020 challengeì—ì„œ 1ìœ„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.<br>

---

## Introduction

monocular 3D object detectionì˜ ê°€ì¥ ì‰¬ìš´ solutionì€ 2D domainì˜ ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¥´ê³ , ë³´ì¡° 3D attibuteë“¤ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ extra componentë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ì „ ë…¼ë¬¸ë“¤ ì¤‘ [30, 20]ì€ 2D box ì˜ˆì¸¡ì„ ìœ ì§€í•˜ê³ , ê±°ê¸°ì— 2D centerë“¤ê³¼ regions of interestì— ëŒ€í•´ 3D attributeë“¤ì„ regress í•˜ëŠ” ë°©ì‹ì´ë‹¤. [1, 9, 2]ëŠ” ê° 2D  anchorë“¤ì— í•´ë‹¹í•˜ëŠ” 3D priorsë¥¼ ì‚¬ìš©í•˜ì—¬ 2Dì™€ 3D boxë“¤ì„ ë™ì‹œì— ì˜ˆì¸¡í•œë‹¤.

ğŸ“ ìœ„ methodì™€ ê´€ë ¨ëœ ë…¼ë¬¸ <br>
<span style='font-size: 10pt'>[30] Andrea Simonelli, Samuel Rota Rota Bulo, Lorenzo Porzi, Manuel Lopez-Antequera, and Peter Kontschieder. Disentangling monocular 3d object detection. In IEEE International Conference on Computer Vision, 2019.<br>
<span style='font-size: 10pt'>[20] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi10d: Monocular lifting of 2d detection to 6d pose and metric shape. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.  
<span style='font-size: 10pt'>[1] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal network for object detection. In IEEE International Conference on Computer Vision, 2019.  
<span style='font-size: 10pt'>[9] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.  
<span style='font-size: 10pt'>[2] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele. Kinematic 3d object detection in monocular video.In Proceedings of the European Conference on Computer Vision, 2020.  

3D Object Detectionì—ì„œ í•´ê²°í•´ì•¼ í•  ë¬¸ì œëŠ” 2D-3D correspondenceë¡œ 3D targetì„ 2D domainì— í• ë‹¹í•˜ê³  ë‚˜ì¤‘ì— ì˜ˆì¸¡ì„ í•˜ëŠ” ë°©ì‹ì„ ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€ ì…ë‹ˆë‹¤.FCOS3Dì—ì„œ ì œì•ˆí•œ 2D detectorë¡œ 3D localizationì„ ì˜ˆì¸¡í•œ ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

  1. 2D image ìƒì— 7-DoF 3D locationsë¥¼ projectí•œ í›„, projectëœ center pointë¥¼ 3D-centerë¼ê³  ì •ì˜í•¨. ì´ projectionì„ í†µí•´, 3D-centerëŠ” 2.5Dì˜ ì •ë³´(2D locationê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” depth)ë¥¼ ê°–ê²Œ ë¨.

  2. 2D locationì€ imageì˜ íŠ¹ì • ì§€ì ì—ì„œ 2D offsetìœ¼ë¡œ ë” ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” 2D detectionì—ì„œì™€ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ feature level ì‚¬ì´ì—ì„œ normalize ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìœ ì¼í•œ 2D attribute ì—­í• ì„ í•¨. ê·¸ì™€ ë‹¬ë¦¬, depth, 3D size, orientationì€ decoupling ì´í›„ì˜ 3D attributeì´ ë¨. ì´ ë°©ë²•ìœ¼ë¡œ, 3D targetë“¤ì„ center-based paradigmìœ¼ë¡œ ë³€í™˜í•˜ì—¬, ì´ì „ ë°©ì‹ì¸ 2D detectionì´ë‚˜ 2D-3D correspondenceë¥¼ ì ìš©í•˜ì§€ ì•Šì•„ë„ ë¨.

êµ¬í˜„ì€ ì´ë¦„ì²˜ëŸ¼ FCOSë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤.
FCOSì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ìë©´, FCOSëŠ” simple anchor-free fully convolutional single-stage detector ì…ë‹ˆë‹¤.
ê°€ì¥ ë¨¼ì €, objectë“¤ì„ 2D scaleì„ ê³ ë ¤í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ feature level(P3 ~ P7)ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ, ê° training sampleì˜ regression targetë“¤ì„ ì¼ì¹˜í•˜ëŠ” projected 3D centerì—ë§Œ assigní•©ë‹ˆë‹¤.
FCOSì™€ FCOS3Dì˜ ì°¨ì´ì ì€ FCOSëŠ” center-nessë¥¼ boundaryì˜ distanceë¡œ ë‚˜íƒ€ë‚¸ ë°˜ë©´, FCOS 3DëŠ” 3D-centerë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ 2D ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ 3D center-nessë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.

---

## Approach

ë‹¤ë£° ë‚´ìš©

> 1. frameworkì˜ overview
> 2. taskì— ë§ê²Œ ìˆ˜ì •ëœ 2D guided multi-level 3D predictionê³¼ 3D center-ness with 2D Gaussian distributionì— í•´ë‹¹í•˜ëŠ” technical designì— ëŒ€í•œ ì„¤ëª…

### - Framework Overview

fully convolutional one-stage detectorì˜ ì„¸ ê°€ì§€ êµ¬ì„±ìš”ì†Œ: feature extractionì„ í•˜ê¸° ìœ„í•œ backbone, multi-level branches êµ¬ì¡°ë¥¼ ìœ„í•œ necks, dense predictionì„ ìœ„í•œ detection heads.

- Backbone: feature extractionì„ ìœ„í•´ pretrained ResNet101 with deformable convolutions ì„ backboneìœ¼ë¡œ ì‚¬ìš©í•¨.  memory overheadë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì²«ë²ˆì§¸ convolutional blockì˜ parameterë¥¼ ê³ ì •í•´ì„œ ì‚¬ìš©í•¨.
- Neck: Feature Pyramid Network(FPN)ì„ ì‚¬ìš©í•˜ì—¬  different scaleì—ì„œ objectë¥¼ detecting í•¨. original FCOSì²˜ëŸ¼ P3 ~ P5ë¥¼ ë§Œë“  í›„, P5ë¥¼ downsampling í•˜ì—¬ P6, P7ì„ ë§Œë“¦.
- Detection Head: shared detection headë¥¼ ì‚¬ìš©í•  ê²½ìš°, ë‘ ê°€ì§€ ë¬¸ì œê°€ ìˆìŒ. ì²«ë²ˆì§¸ëŠ”, targetì„ ë‹¤ë¥¸ feature levelê³¼ pointë“¤ë¡œ distributioní•˜ëŠ” ë°©ë²•ì„. ì´ ë¬¸ì œì— ëŒ€í•´ì„œëŠ” 3.2ì—ì„œ ë‹¤ë£¸. ë‘ë²ˆì§¸ ë¬¸ì œëŠ”, architectureë¥¼ ë””ìì¸í•˜ëŠ” ë°©ë²•ì„. ê¸°ë³¸ì ì¸ ë””ìì¸ì€ RetinaNetê³¼ FCOSë¥¼ ë”°ë¦„. ê° shared headëŠ” 4 shared convolutional blockê³¼ ì„œë¡œ ë‹¤ë¥¸ targetì— ëŒ€í•œ small headë¡œ êµ¬ì„±ë¨. measurementê°€ ë‹¤ë¥¸ regression targetì— ëŒ€í•´ì„œ ë¶„ë¦¬ëœ headë¥¼ ë‘ëŠ” ê²ƒì´ ê²½í—˜ì ìœ¼ë¡œ ë” íš¨ê³¼ì ì´ì–´ì„œ ê° targetë§ˆë‹¤ ì‘ì€ headë¥¼ í•˜ë‚˜ì”© ë§Œë“¦.
- Regression Targets

  FCOSì—ì„œ ë‚˜ì˜¨ anchor-freeì˜ ê°œë…ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

  - $F_i \in \mathbb{R}^{H \times W \times C}$ ë¡œ ì •ì˜ëœ backboneì˜ ië²ˆì§¸ layerì—ì„œ ì–»ì€ feature mapì´ ì£¼ì–´ì§€ë©´, feature mapì˜ ê° pointë¡œ objectë¥¼ predictí•´ì•¼ í•˜ë©°, ì´ëŠ” ì›ë˜ input imageì˜ ê· ì¼í•˜ê²Œ ë¶„í¬ëœ pointì— í•´ë‹¹í•©ë‹ˆë‹¤.
  - feature map $F_i$ ìƒì˜ location $(x, y)$ ì— ëŒ€í•´ layer $i$ ê¹Œì§€ì˜ total strideê°€ $s$ ë¼ ê°€ì •í•˜ë©´, original imageì—ì„œì˜ locationì€ $(sx + \lfloor{s\over2}\rfloor, sy + \lfloor{s\over2}\rfloor)$
  - anchor-based detectorê°€ ë¯¸ë¦¬ ì •ì˜í•œ anchorë¡œ targetì„ regressing í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, FCOSëŠ” ì´ locationìœ¼ë¡œ ë°”ë¡œ objectë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. (anchor-free)
  - ë” ì´ìƒ anchorë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, pointê°€ foregroundì— ì†í•˜ëŠ”ì§€ ì—¬ë¶€ëŠ” GTì™€ì˜ IoU(Intersection over Union)ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ <mark style='background-color: #ffd33d'>box centerì™€ ê°€ê¹Œìš´ ì§€ ì—¬ë¶€ë¡œ íŒë‹¨</mark>í•©ë‹ˆë‹¤.  
  
  <center> <img src="../../assets/images/posts/3DOD/2022-06-24-FCOS3D/3DOD_fcos3d_fig1.JPG" width="700" alt="{{ include.description }}">
    <figcaption style="text-align:center; font-size:16px; color:#808080"> Fig1. FCOSì˜ 2D detection vs. FCOS3Dì˜ 3D detection
    </figcaption>
    </center> 
  <br>
  ê·¸ë¦¼ê³¼ ê°™ì´ 2Dì˜ ê²½ìš°, pointì™€ top/bottom/left/right ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ regress í•´ì•¼í•©ë‹ˆë‹¤.  
  ê·¸ëŸ¬ë‚˜ 3Dì˜ ê²½ìš°, 6ê°œì˜ ê±°ë¦¬(W, H, L, d, $\theta$, ($\Delta x, \Delta y$))ë¥¼ regress í•´ì•¼í•©ë‹ˆë‹¤. (W, H, L: 3D size, $\theta$: orientation, ($\Delta x, \Delta y$): a transformed 3D-center)  
  ì´ë¥¼ ì‰½ê²Œ êµ¬í˜„í•˜ê¸° ìœ„í•´, 7-DoF regression targetì„ 2.5D centerì™€ 3D sizeë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 2.5D centerëŠ” camera intrinsic matrixë¥¼ ì‚¬ìš©í•˜ë©´ 3D ê³µê°„ìœ¼ë¡œ ì‰½ê²Œ ë‹¤ì‹œ ë³€í™˜ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, 2.5D centerë¥¼ regressingí•˜ëŠ” ê²ƒì„ centerì—ì„œ íŠ¹ì • foreground point, $\Delta x, \Delta y$, ë° ê·¸ì— í•´ë‹¹í•˜ëŠ” Depth dë¡œ offsetì„ íšŒê·€ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ë”ìš± ë‹¨ìˆœí•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


  allocentric orientationì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´, angle $\theta$ ì™€ 2-bin direction classificationì„ ì •ì˜í•©ë‹ˆë‹¤.
  <br>
  ìš°ì„  allocentric orientationì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ìë©´, 
  <center> <img src="../../assets/images/posts/3DOD/2022-06-24-FCOS3D/3DOD_fcos3d_fig2.png" width="700" alt="{{ include.description }}">
    <figcaption style="text-align:center; font-size:16px; color:#808080"> Fig2. egocentric orientation vs. allocentric orientation
    </figcaption>
    </center> 
  <br>
  egocentric orientationì€ objectì˜ global orientationì„ ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤. allocentric orientationì€ objectê°€ camera ìƒì—ì„œ ë³´ì´ëŠ” ê°ë„, ì¦‰ local orientationì´ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.
  <br>
  ê·¸ë¦¼ì—ì„œ ë³´ë©´, (a)ì˜ ê²½ìš° ì°¨ê°€ ê³„ì† ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì§„í–‰í•˜ë¯€ë¡œ ì°¨ì˜ global orientationì€ ë™ì¼í•œ ë°˜ë©´, camera ìƒì—ì„œ ë³´ì´ëŠ” ì°¨ì˜ angleì€ ë‹¬ë¼ì§€ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, local orientation, ì¦‰, allocentric orientationì€ ì°¨ê°€ ì´ë™í•¨ì— ë”°ë¼ ë°”ë€ë‹ˆë‹¤. (b)ëŠ”, ì°¨ì˜ global orientationì´ ê³„ì† ë°”ë€ŒëŠ” ë°˜ë©´, camera ìƒì—ì„œ ë³´ì´ëŠ” ì°¨ì˜ orientationì€ ê°™ìœ¼ë¯€ë¡œ local orientationì€ ë™ì¼í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  <center> <img src="../../assets/images/posts/3DOD/2022-06-24-FCOS3D/3DOD_fcos3d_fig3.png" width="700" alt="{{ include.description }}">
    <figcaption style="text-align:center; font-size:16px; color:#808080"> Fig3. egocentric orientation vs. allocentric orientation
    </figcaption>
    </center> 
  <br>

  ë”°ë¼ì„œ, ìœ„ ê·¸ë¦¼ì˜ ê²½ìš° objectì˜ egocentri orientationì€ ë™ì¼í•˜ì§€ë§Œ, allocentric orientationì€ ê³„ì† ë°”ë€ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>
  (ì°¸ê³ : <https://towardsdatascience.com/orientation-estimation-in-monocular-3d-object-detection-f850ace91411>)
  <br>
  ë‹¤ì‹œ ë³¸ë¡ ìœ¼ë¡œ ëŒì•„ì˜µì‹œë‹¤.
  FCOS3Dì—ì„œëŠ” allocentric orientationì„ predictioní•˜ê¸° ìœ„í•´ $\theta$ì™€ 2-bin direction classificationì„ ì •ì˜í•©ë‹ˆë‹¤. <br>
  $\theta$ì˜ ê²½ìš° GT bboxì™€ prediction bbox ê°„ì˜ IoUì„ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤.
  2-bin directionì€ 2ê°œì˜ boxê°€ ë°˜ëŒ€ ë°©í–¥ì„ í–¥í•˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ì •ë³´ì…ë‹ˆë‹¤.
  

<br>

> Paper Link: [<https://arxiv.org/pdf/2104.10956.pdf>]
